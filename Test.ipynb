{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Groq API for High-Speed LLM Inference\n",
    "\n",
    "This Jupyter Notebook provides a comprehensive and robust guide on how to use the Groq API to interface with Large Language Models (LLMs). Groq is known for its exceptionally fast inference speeds, making it a powerful tool for building responsive and real-time AI applications.\n",
    "\n",
    "We will cover the following topics:\n",
    "1.  **Prerequisites**: What you need to get started.\n",
    "2.  **Setup**: Installing the necessary library and configuring your environment.\n",
    "3.  **Basic Usage**: Making your first API call to the Groq API.\n",
    "4.  **Advanced Usage**:\n",
    "    *   Streaming responses for a real-time feel.\n",
    "    *   Using the asynchronous client for concurrent requests.\n",
    "    *   Controlling the LLM's output with parameters.\n",
    "5.  **Error Handling**: How to gracefully handle potential API errors.\n",
    "6.  **Putting It All Together**: A simple command-line chatbot example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "Before you can use the Groq API, you need to have a Groq account and an API key.\n",
    "\n",
    "1.  **Create a Groq Account**: If you don't have one already, sign up for a free account on the [Groq website](https://groq.com/).\n",
    "2.  **Generate an API Key**: Navigate to the API Keys section in your GroqCloud dashboard and create a new API key.\n",
    "\n",
    "**Important**: Your API key is a secret! Do not share it publicly or commit it to version control. The best practice is to store it as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "First, you need to install the official Groq Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Environment Variable\n",
    "\n",
    "To keep your API key secure, we'll use an environment variable named `GROQ_API_KEY`. You can set this in your operating system, or for the purpose of this notebook, we'll use the `os` library to set it for the current session. \n",
    "\n",
    "**Note:** For a more permanent solution, consider using a `.env` file and the `python-dotenv` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "#it's recommended to set the API key as an environment variable for security\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass(\"Enter your Groq API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Usage: Your First API Call\n",
    "\n",
    "Now that the setup is complete, let's make a simple request to the Groq API to get a chat completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    #api_key is read from the GROQ_API_KEY environment variable by default\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Identify three key problems in the clinical space from a patient's perspective.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response Object\n",
    "\n",
    "The response from the `create` method is an object containing useful information. Let's inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key fields in the response include:\n",
    "- `id`: A unique identifier for the completion.\n",
    "- `object`: The type of object, which is `chat.completion`.\n",
    "- `created`: A Unix timestamp of when the completion was created.\n",
    "- `model`: The model used for the completion.\n",
    "- `choices`: A list of completion choices. You can request more than one, but typically you'll work with the first one.\n",
    "    - `message`: The message object containing the `role` ('assistant') and the `content` (the actual response).\n",
    "    - `finish_reason`: The reason the model stopped generating text (e.g., 'stop').\n",
    "- `usage`: Information about the number of tokens used for the prompt and the completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Responses\n",
    "\n",
    "For interactive applications like chatbots, you'll want to display the response as it's being generated. This is achieved through streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Come up with a list of ideas for a clinically focused hackathon.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Client\n",
    "\n",
    "For applications that require high concurrency (e.g., a web server handling multiple user requests at once), you should use the asynchronous client. This allows your program to make multiple API calls without waiting for each one to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from groq import AsyncGroq\n",
    "\n",
    "async_client = AsyncGroq()\n",
    "\n",
    "async def main():\n",
    "    stream = await async_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the main difference between a synchronous and an asynchronous API call?\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    async for chunk in stream:\n",
    "        print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "\n",
    "# To run the async function in a Jupyter Notebook\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the LLM's Output\n",
    "\n",
    "You can influence the behavior of the LLM by adjusting several parameters in the `create` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlled_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Come up with an idea for a clinically focused hackathon.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    "    \n",
    "    #controls randomness. A lower value makes the model more deterministic.\n",
    "    temperature=0.9,\n",
    "    \n",
    "    # The maximum number of tokens to generate.\n",
    "    max_tokens=1024,\n",
    "    \n",
    "    #the model considers the results of the tokens with top_p probability mass.\n",
    "    top_p=1,\n",
    "    \n",
    "    #sequence where the API will stop generating further tokens.\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "print(controlled_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Handling\n",
    "\n",
    "It's crucial to handle potential errors when making API calls, such as network issues, invalid API keys, or invalid requests. The Groq library raises specific exceptions for different types of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq, APIStatusError, APIConnectionError\n",
    "\n",
    "bad_client = Groq(api_key=\"invalid-api-key\")\n",
    "\n",
    "try:\n",
    "    chat_completion = bad_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"This request will fail.\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "except APIStatusError as e:\n",
    "    print(f\"API Status Error: {e.status_code} {e.response}\")\n",
    "except APIConnectionError as e:\n",
    "    print(f\"API Connection Error: {e.__cause__}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together: A Simple Chatbot\n",
    "\n",
    "Let's create a simple command-line chatbot that maintains a conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def chatbot():\n",
    "    client = Groq()\n",
    "    \n",
    "    conversation_history = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly and helpful chatbot. Keep your answers concise.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"Chatbot initialized. Type 'exit' to end the conversation.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() == 'exit':\n",
    "                print(\"Chatbot shutting down. Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            \n",
    "            stream = client.chat.completions.create(\n",
    "                messages=conversation_history,\n",
    "                model=\"llama3-8b-8192\",\n",
    "                stream=True,\n",
    "            )\n",
    "            \n",
    "            print(\"Groq: \", end=\"\")\n",
    "            full_response = \"\"\n",
    "            for chunk in stream:\n",
    "                response_chunk = chunk.choices[0].delta.content or \"\"\n",
    "                print(response_chunk, end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                full_response += response_chunk\n",
    "            \n",
    "            print() # for a new line\n",
    "            \n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nChatbot shutting down. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "#to run the chatbot, uncomment the line below and run the cell.\n",
    "#chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Further Resources\n",
    "\n",
    "For more detailed information, refer to the official Groq documentation:\n",
    "\n",
    "*   [Groq API Documentation](https://console.groq.com/docs)\n",
    "*   [Groq Python GitHub Repository](https://github.com/groq/groq-python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
